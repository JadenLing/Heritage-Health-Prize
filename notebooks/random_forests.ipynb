{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# For parallel processing (-1 means use all cores, None means 1 core)\n",
    "PARALLEL = 10\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('../data/preprocessed/prp_combined_Y1.csv')\n",
    "test = pd.read_csv('../data/preprocessed/prp_combined_Y2.csv')\n",
    "\n",
    "# Segment data\n",
    "Y_train = train['DaysInHospitalY2']\n",
    "X_train = train.drop(columns=['DaysInHospitalY2', 'MemberID'])\n",
    "Y_test = test['DaysInHospitalY3']\n",
    "X_test = test.drop(columns=['DaysInHospitalY3', 'MemberID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forests Classifier**\n",
    "This notebook utilises a Random Forests Classifier to determine `DaysInHospital`. The problem is modelled as a classification task with 16 classes (0 - 15).\n",
    "Hyperparameters are tested using a custom implementation of random search hyperparameter tuning (given the number of parameters, it would be infeasible to test using Grid Search), with the best estimator being selected via `roc_auc` score.\n",
    "Stratified K-Fold is applied to preserve class distributions (important since dataset is imbalanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions and parameter initializations\n",
    "\n",
    "# Stratified K-Fold\n",
    "outer_split = StratifiedKFold(n_splits=20, shuffle=True, random_state=42)\n",
    "inner_split = StratifiedKFold(n_splits=4)\n",
    "\n",
    "# Hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100, 150],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 4, 8],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Classifier\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Number of random search samplings\n",
    "N_ITER = 10\n",
    "\n",
    "# Normalisation scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "cv_scores = {\n",
    "    'roc_auc': [],\n",
    "    'f1': [],\n",
    "    'precision': [],\n",
    "    'recall': []\n",
    "}\n",
    "\n",
    "best_params = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross Validation and Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary cross validation loop\n",
    "i = 1\n",
    "start = time.time()\n",
    "print(\"Beginning cross validation loop\")\n",
    "for outer_train_idx, outer_test_idx in outer_split.split(X_train, Y_train):\n",
    "\n",
    "    X_train_outer, X_test_outer = X_train.iloc[outer_train_idx], X_train.iloc[outer_test_idx]\n",
    "    Y_train_outer, Y_test_outer = Y_train.iloc[outer_train_idx], Y_train.iloc[outer_test_idx]\n",
    "\n",
    "    # Normalise length of stay within CV loop\n",
    "    X_train_outer['LengthOfStay'] = scaler.fit_transform(X_train_outer[['LengthOfStay']])\n",
    "    X_test_outer['LengthOfStay'] = scaler.transform(X_test_outer[['LengthOfStay']])\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    hyper_scoring = {\n",
    "        'mean_roc_aucs': [],\n",
    "        'params': []\n",
    "    }\n",
    "    # Sampling hyperparameters\n",
    "    for k in range(N_ITER):\n",
    "        #print(f\"Inner sampling loop {k+1}\")\n",
    "\n",
    "        sum_roc_auc = 0\n",
    "        params = {\n",
    "            'n_estimators': param_grid['n_estimators'][np.random.randint(0, len(param_grid['n_estimators']))],\n",
    "            'max_depth': param_grid['max_depth'][np.random.randint(0, len(param_grid['max_depth']))],\n",
    "            'min_samples_split': param_grid['min_samples_split'][np.random.randint(0, len(param_grid['min_samples_split']))],\n",
    "            'min_samples_leaf': param_grid['min_samples_leaf'][np.random.randint(0, len(param_grid['min_samples_leaf']))],\n",
    "            'criterion': param_grid['criterion'][np.random.randint(0, len(param_grid['criterion']))]\n",
    "        }\n",
    "\n",
    "        # Secondary (inner) cross validation loop\n",
    "        for inner_train_idx, inner_test_idx in inner_split.split(X_train_outer, Y_train_outer):\n",
    "            X_train_inner, X_test_inner = X_train_outer.iloc[inner_train_idx], X_train_outer.iloc[inner_test_idx]\n",
    "            Y_train_inner, Y_test_inner = Y_train_outer.iloc[inner_train_idx], Y_train_outer.iloc[inner_test_idx]\n",
    "\n",
    "            rfc.set_params(**params)\n",
    "            rfc.fit(X_train_inner, Y_train_inner)\n",
    "\n",
    "            Y_pred_proba_inner = rfc.predict_proba(X_test_inner)\n",
    "            sum_roc_auc += roc_auc_score(Y_test_inner, Y_pred_proba_inner, multi_class='ovr')\n",
    "\n",
    "        hyper_scoring['mean_roc_aucs'].append(sum_roc_auc / inner_split.get_n_splits())\n",
    "        hyper_scoring['params'].append(params)\n",
    "\n",
    "    # Determine best estimator based on roc_auc\n",
    "    best_hyper_params = hyper_scoring['params'][np.argmax(hyper_scoring['mean_roc_aucs'])]\n",
    "    best_params.append(best_hyper_params)\n",
    "\n",
    "    # Refit model with outer loop training data\n",
    "    rfc.set_params(**best_hyper_params)\n",
    "    rfc.fit(X_train_outer, Y_train_outer)\n",
    "\n",
    "    Y_pred = rfc.predict(X_test_outer)\n",
    "    Y_pred_proba = rfc.predict_proba(X_test_outer)\n",
    "\n",
    "    # Calculate scores and save them\n",
    "    cv_scores['roc_auc'].append(roc_auc_score(Y_test_outer, Y_pred_proba, multi_class='ovr'))\n",
    "    cv_scores['precision'].append(precision_score(Y_test_outer, Y_pred, average='macro', zero_division=1))\n",
    "    cv_scores['recall'].append(recall_score(Y_test_outer, Y_pred, average='macro', zero_division=1))\n",
    "    cv_scores['f1'].append(f1_score(Y_test_outer, Y_pred, average='macro', zero_division=1))\n",
    "\n",
    "    print(f\"Outer loop {i}, time elapsed: {(time.time() - start) / 60:.2f} minutes\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best estimator among best estimators using greatest roc_auc\n",
    "hold_x_train, hold_x_test, hold_y_train, hold_y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "hold_x_train['LengthOfStay'] = scaler.fit_transform(hold_x_train[['LengthOfStay']])\n",
    "hold_x_test['LengthOfStay'] = scaler.transform(hold_x_test[['LengthOfStay']])\n",
    "\n",
    "best = {\n",
    "    'best_roc_auc': 0,\n",
    "    'best_estimator': None\n",
    "}\n",
    "for params in best_params:\n",
    "    rfc.set_params(**params)\n",
    "    rfc.fit(hold_x_train, hold_y_train)\n",
    "\n",
    "    pred = rfc.predict(hold_x_test)\n",
    "    pred_proba = rfc.predict_proba(hold_x_test)\n",
    "\n",
    "    roc_auc = roc_auc_score(hold_y_test, pred_proba, multi_class='ovr')\n",
    "    \n",
    "    if best['best_roc_auc'] < roc_auc:\n",
    "        best['best_roc_auc'] = roc_auc\n",
    "        best['best_params'] = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Results and Visuals Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final summary scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score final model and get predictions on test set\n",
    "params = best['best_params']\n",
    "\n",
    "rfc.set_params(**best['best_params'])\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "pred = rfc.predict(X_test)\n",
    "pred_proba = rfc.predict_proba(X_test)\n",
    "\n",
    "roc_auc = roc_auc_score(Y_test, pred_proba, multi_class='ovr')\n",
    "precision = precision_score(Y_test, pred, average='macro', zero_division=1)\n",
    "recall = recall_score(Y_test, pred, average='macro', zero_division=1)\n",
    "f1 = f1_score(Y_test, pred, average='macro', zero_division=1)\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross Validation Summary Score Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard deviations for scores\n",
    "std_roc_auc = np.std(cv_scores['roc_auc'])\n",
    "std_precision = np.std(cv_scores['precision'])\n",
    "std_recall = np.std(cv_scores['recall'])\n",
    "std_f1 = np.std(cv_scores['f1'])\n",
    "\n",
    "stds = [std_roc_auc, std_precision, std_recall, std_f1]\n",
    "\n",
    "# Means of scores\n",
    "mean_roc_auc = np.mean(cv_scores['roc_auc'])\n",
    "mean_precision = np.mean(cv_scores['precision'])\n",
    "mean_recall = np.mean(cv_scores['recall'])\n",
    "mean_f1 = np.mean(cv_scores['f1'])\n",
    "\n",
    "means = [mean_roc_auc, mean_precision, mean_recall, mean_f1]\n",
    "\n",
    "colours = ['#ADD8E6', '#90EE90', '#F08080', '#FFA500']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "plt.bar(['ROC AUC', 'Precision', 'Recall', 'F1'], means, yerr=stds, color=colours, capsize=5)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Random Forest Classifier Cross Validation Scores')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Permutation feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Feature Importance\n",
    "res = permutation_importance(rfc, X_train, Y_train, n_repeats=30, random_state=42, n_jobs=PARALLEL)\n",
    "feature_importance = pd.Series(res.importances_mean * 1e4, index=X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "feature_importance.plot.bar(yerr=res.importances_std * 1e4, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation on full model\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease (1e-4)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ROC-AUC Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_binary_labels = label_binarize(Y_test, classes=np.unique(Y_test))\n",
    "\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "for i in range(np.unique(Y_test).size):\n",
    "    display = RocCurveDisplay.from_predictions(\n",
    "        true_binary_labels[:, i],\n",
    "        pred_proba[:, i], \n",
    "        ax=ax_roc, \n",
    "        name=f\"Class {i}\",\n",
    "        plot_chance_level=(i == np.unique(Y_test).size - 1)\n",
    "    )\n",
    "\n",
    "_ = ax_roc.set(\n",
    "    xlabel='False Positive Rate (FPR)',\n",
    "    ylabel='True Positive Rate (TPR)',\n",
    "    title='ROC Curve for Random Forest Classifier'\n",
    ")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(fontsize='8', loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision-Recall Curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_prc, ax_prc = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "for i in range(np.unique(Y_test).size):\n",
    "    display = PrecisionRecallDisplay.from_predictions(\n",
    "        true_binary_labels[:, i],\n",
    "        pred_proba[:, i], \n",
    "        ax=ax_prc, \n",
    "        name=f\"Class {i}\",\n",
    "        plot_chance_level=(i == np.unique(Y_test).size - 1)\n",
    "    )\n",
    "\n",
    "_ = ax_prc.set(\n",
    "    xlabel='Recall',\n",
    "    ylabel='Precision',\n",
    "    title='Precision-Recall Curve for Random Forests Classifier'\n",
    ")\n",
    "\n",
    "plt.legend(fontsize='8', loc='upper right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
